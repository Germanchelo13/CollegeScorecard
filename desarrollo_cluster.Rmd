---
title: "Informe college scorecard"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    theme: paper

    fig_width: 8
    fig_height: 8
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(kableExtra)
```




```{css echo=FALSE, language="css"}
h1  {
  text-align: center;
  color: navy;
  text-decoration: underline #BE05FA;
}
h2 {
  text-align: center;
  color: blue;
  text-decoration: underline #BE05FA;
}
h3 {
  text-align: center;
  color: blue;
  text-decoration: underline #BE05FA;
}
body {
  color: black;
  counter-reset:section;
}
figure {
  border: 2px dashed red;
  margin: 1em 0;
}
figcaption {
  padding: .5em;
  font-size: 1.3em;
  text-align: center;
}

```


**Tecnicas de aprendizaje estadistico**

**Integrantes**

*   David Andres Cano Gonzalez
*   David Garcia Blandon
*   German Alonso Patino Hurtado
*   Juan Pablo Buitrago Diaz

# Introducción



Para este trabajo se considero una base de datos que se encuentra en <a href= 'https://data.world/exercises/cluster-analysis-exercise-2' 
  target='_blank'> CollegeScorecard </a> que cuenta con información de 7804 universidades en 
  Estados Unidos con cerca de 1000 columnas. A la hora de buscar una universidad existen multiples factores a considerar como la ubicación de esta y su nivel educativo, esta ultima puede ser algo complejo y difícil de medir ya que dependiento de la persona considera si una universidad es apropiada para él o sostenible.

# Objetivo

- Ofrecer un aplicativo para que el usuario tenga una alternativa para encontrar  universidades.

- Situar las universidades respecto a la ubicación que desea el usuario y además sea acorde a sus necesidades.  


# Exploración de datos

Para este trabajo se cuenta con los datos de CollegeScorecard.csv donde College scorecard es una herramienta en línea fue creada por el gobierno de Estados Unidos con el fin de que los usuarios consulten el costo de las universidades y muchas caracteristicas pero tal vez no es una manera dinameca de encontrar una universidad. Los datos cuentan con información de 7804 escuales de eduación superior con 1725 elementos por escuela. En <a href= 'https://data.world/exercises/cluster-analysis-exercise-2' 
  target='_blank'> CollegeScorecard </a>  se puede encontrar un documento   __FullDataDocumentation.pdf__ que cuenta con una descripción general de las columnas del data set, después de explorar datos y definiciones de las variables se escogieron las siguientes variables.



## Seleccion de variables

* ID: Existen varios tipos de identificadores en los datos que son:
  - UNITID, OPEID,  opeid6.
  -	INSTNM: nombre de la institución (y campus).
* Campus: La ubicación de la insitución corresponde a un campus y se tiene:
  - NUMBRANCH: Número de campus que cuenta la universidad.
  - main: 1 si el campus es el principal 0 sino lo es.
* Localización: se tiene la lat y long, ciudad, sta
  - CITY: Ciudad.
  - STABBR: Estado 2 caracteres.
  - ZIP: Código postal.
  - LATITUDE: Latitud.
  - LONGITUDE: Longitud.
* URL: Se tienen las URL de los datos de 2013.
  - INSTURL: Pagina web universidad.
  - NPCURL: URL costos netos de la universidad.
* Titulación: El nivel de titulación clasificado en 5 niveles del 0 al 4 (posgrado), una universidad puede ofrecer varios tipos de titulación y se tienen las siguientes variables: 
  - HIGHDEG: Nivel mas alto que ofrece la universidad.
  - PREDDEG: Nivel mas predominante (mayor proporción en titulación).
* Tipo de universidad:
  - CONTROL: 1: publica, 2: privada sin animo de lucro, 3: privada con animo de lucro.
* Costos: Metricas de costos por estudiantes:
  - TUITFTE: Ingresos por matrícula menos los descuentos y bonificaciones, y los dividen por el número de estudiantes (costo de matricula promedio por estudiante).
  - INEXPFTE: Gastos de instrucción divididos por el número de estudiantes a tiempo completo.
* Problemas financieros:
  - HCM: 1 si tiene problemas financieros 0 sino.
* Programas:
  - CIP (190): identifica si la institución ofrece el programa, en qué nivel, y si la institución ofrece el programa y el nivel a través de un programa exclusivamente de educación a distancia. Se calcula a partir de los recuentos de adjudicaciones realizadas en cada CIP, tal y como se informa en IPEDS (el tipo de programa es solo virtual si vale 2, se desconoce el significado de los demás niveles).


# Elaboración de cluster

## Requerimientos


Este analisis se hara en python y se usaron las siguientes librerias:

- **pandas**: Manejo de dataframe.
- **numpy**: Manejo matricial.
- **seaborn**: Realización de gráficos.
- **plotly.express**: Plot mapa interactivo.
- **sklearn.cluster**: Algoritmos de cluster, documentación: https://scikit-learn.org/stable/modules/clustering.html 
- **sklearn.decomposition**: Aplicación de PCA.


```{python id="YKGax0S7y85E"}
import pandas as pd
import numpy as np
import seaborn as sns 
# import plotly.express as px
import sklearn.cluster 
import sklearn.decomposition
import matplotlib.pyplot as plt
# Cantidad de filas y columnas maximas a imprimir
pd.set_option("display.max_columns", 500)
pd.set_option("display.max_rows", 500)
import funciones as fn
import warnings
warnings.filterwarnings('ignore')
```

```{python}
def kmeans(datos_:pd.DataFrame,variables_:list ,n_:int,semilla:int  )->list:
    """
    Aplica el algoritmo
    datos: pd.DataFrame.
    variables: list Lista de variables que se incluyen kmeans (deben ser numericas).
    n_: int número de cluster.
    semilla: int semilla de aleatoriedad. 
    return: list de los segmentos, numerico.
    """
    X=datos_[variables_].copy()
    cluster_kmeans= sklearn.cluster.KMeans( n_clusters=n_, random_state=semilla).fit(X)
    segmentos_=cluster_kmeans.labels_

    return segmentos_

def spectral(datos_:pd.DataFrame, variables_:list, n_:int,semilla:int )->list:
    """
    Aplica la función EspectralClustering para la asignación de cluster.
    Aplica el algoritmo
    datos: pd.DataFrame.
    variables: list Lista de variables que se incluyen kmeans (deben ser numericas).
    n_: int número de cluster.
    semilla: int semilla de aleatoriedad. 
    return: list de los segmentos, numerico.  
    """
    X=datos_[variables_].copy()
    cluster_spectral=sklearn.cluster.SpectralClustering(
      n_clusters=n_,
      assign_labels='discretize',
      random_state=semilla).fit(X)
    segmentos_=cluster_spectral.labels_

    return segmentos_

def afinitypropagation(datos_:pd.DataFrame, variables_:list, damp:float(0.5),semilla:int)->np.array:
    """
    Aplica el algoritmo de AffinityPropagation.
    datos: pd.Dataframe
    variables: list lista de string con el nombre de variables usadas en el algoritmo.
    damp: float [0.5,1)
    semilla: int de la asignacion aleatoria. 
    return: np.array labels.
    """
    X=datos_[variables_].copy()
    cluster_afiniti_propagation= sklearn.cluster.AffinityPropagation(damping=damp,random_state=5).fit(X)
    segmentos_=cluster_afiniti_propagation.labels_
    return segmentos_

def estandarizacion(datos_:pd.DataFrame,variables_numeric:list  )->pd.DataFrame:
    """
    Estandariza las variables de un dataframe para que esten entre 0 y 1
    datos: pd.DataFrame.
    variables_numeric: Lista de variables que se quieren estandarizar 
    """
    datos_new=datos_.copy()
    valores_min=datos_new[variables_numeric].min(axis=0)
    valores_max=datos_new[variables_numeric].max(axis=0)
    datos_new[variables_numeric]=(datos_new[variables_numeric]- valores_min)/(valores_max-valores_min )

    return datos_new

def to_dummy(datos_:pd.DataFrame, variables_dumy:list)->pd.DataFrame:
    """
    Transoforma variables categoricas a dummy, solo las variables ingresadas.
    datos_: pd.DataFrame
    varaibles_dumy: list de variables categoricas que se van a crear transformar a dummy
    return: pd.DataFrame con todas las variables de datos y las variables_dumy remplezadas con la trasnformación.
    """
    for var in variables_dumy:
        dummy_temp=pd.get_dummies(datos_[var],prefix=var ).copy()
        columns_temp=dummy_temp.columns
        num_columns=len(columns_temp)
        datos_=pd.concat([datos_.drop(labels=var,axis=1),dummy_temp[columns_temp[0:num_columns-1]  ] ],axis=1)

    return datos_

# def datos_to_pca(datos_:pd.DataFrame, variables_:list, prop_var:float)->pd.DataFrame:
#     """
#     Devuelve el numero de componentes que expliquen un prop_var*100% de variabilidad. 
#     datos_: pd.DataFrame (se recomienda que esten estandarizados).
#     variables_: list nombre de las variables que se les aplicara el PCA.
#     prop_var: float proporción de variabilidad que se quiere explicar.
#     return: pd.DataFrame con las componentes que explican al menos un % prop varianzas.
#     """
#     pca_=sklearn.decomposition.PCA(len(variables_))
#     pca_.fit(datos_[variables_])
#     varianza_acumulate=pd.Series(pca_.explained_variance_ratio_).cumsum()
#     varianza_acumulate.index=varianza_acumulate.index+1
#     num_componentes=varianza_acumulate[varianza_acumulate>=prop_var].index[0]
#     varianza_acumulate.plot(xlabel='Número de componentes', ylabel='proporción de varibilidad acumulada',grid=True,marker='o')
#     plt.show()
#     #pca_final=sklearn.decomposition.PCA(num_componentes)
#     #pca_final.fit(datos_[variables_])
#     #data_result=pd.DataFrame(pca_final.fit_transform(datos_[variables_]))
#     #return #data_result


```



## Depuración  de datos



En esta sección se analisara posibles datos faltantes, uno de los factores que se encontro es que existen universidades que no tienen publica su información en algunas columnas y los registros estan como 'PrivacySuppressed' esto se va a considerar como un valor faltante  (__NA__).



```{python}
# lectura de datos con URL.
datos=pd.read_csv('https://query.data.world/s/zm2i4iby5glnnw4ktejngj775gg7j3')
# diccionario de los datos 
diccionario = pd.read_csv('https://query.data.world/s/mwzxjzee7zbruhcovlmoq5unjy2xdq')

# guardar variables cip
variables_cip=[]
for i in datos.columns:
    if 'CIP' == i[0:3]:
        variables_cip.append(i)

variables_id= ['UNITID', 'OPEID', 'opeid6','INSTNM', # id 
          'main', # num campus and campus 
           'CITY',  'STABBR',  'ZIP','st_fips','RELAFFIL',
          'LATITUDE','LONGITUDE', # localizacion
          'INSTURL', 'NPCURL'] # URL´s
variables_dum=['CONTROL', # privado publico 
          'HIGHDEG', 'PREDDEG', # nivel formación
          'DISTANCEONLY', # solo programas a distancia
            'HCM2', # riesgo financiero
]
variables_num=[  'TUITFTE', 'INEXPFTE',# 'AVGFACSAL', # financiacion monetaria promedio
]
#*variables_id,
variables=[*variables_dum,*variables_num ]
```



```{python, plot1, fig.caption="Distribución de NA "}
valores_faltantes=datos[variables].isna().sum()+(datos[variables]=='PrivacySuppressed').sum()
na_cip=datos[variables_cip].isna().any(axis=1)
valores_faltantes=valores_faltantes.append(pd.Series({"CIP":na_cip.sum() }))
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x=valores_faltantes.index,height=valores_faltantes)
plt.xlabel("Variables")
plt.ylabel("Cantidad de NA")
plt.title("Total NA de las variables")
plt.show()

```
<figcaption> Fig 1: Distribución de NA </figcaption>

En la Fig 1 se observa la frecuencia de NA por cada variable, en la variable CIP se consideran todas las variables y la frecuencia es que al menos un regitro tiene NA. Para este trabajo se omiten las universidades que con estas variables tengan al menons un NA.

```{python, plot4}
datos_resultado=datos.loc[(~datos[[*variables_cip,*variables]].isna().any(axis=1)) & ((datos[['HIGHDEG', 'PREDDEG']]!=0).any(axis=1))
                          ,[*variables_id,*variables,*variables_cip] ].reset_index().drop(labels='index',axis=1)
```

De las  7804  escuales de educación superior se trabajara solo con  7279  un  93.27 % de las escuales.



```{python }
datos_resultado['NUM_PROGRAM']=(datos_resultado[variables_cip]>0).sum(axis=1)
variables=[*variables,'NUM_PROGRAM']
variables_num=[*variables_num,'NUM_PROGRAM']
```

Para las variables CIP se crea una variable NUM_PROGRAM que es el número de programas que ofrece la universidad, es decir, cada programa se cuenta si la variable CIP tiene 1 o 2 y se obtiene el total en cada uno.

Las variables se clasificaran como:

- variables_id: identificar a las universidades como ubicación, nombre, código id, pagina web (no entran en el desarrollo de cluster).

- variables_dum: Las identificamos como variables categoricas y en su mayoria tienen 2 niveles y no es necesario crear una dummy pues ya estan definias 0,1. Las variables CONTROL, HIGHDEG, PREDDEG se debe crear una dummy para cada una.

- variables_num: 5 variables, una de conteo, 2 de promedio.


## Analisis descriptivo

A continuacion, analizamos las correlaciones entre las variables numericas escogidas.


```{python}
plt.figure(1)
sns.heatmap(datos_resultado[variables_num].corr(), annot=True)
plt.show()
```

<figcaption> Fig 2: Correlación  entre variables  </figcaption>

 
En la Figure Fig 2 se observa que las variables INEXPFTE  y TUITFTE Son las que presentan mayor correlación, siendo esta de 0.38, la cual no es muy alta. 


# Segmentación


## Estandarización y creación de dummy.

- Para las variables numéricas se estandariza para que se encuentren entre 0 y 1 $\frac{X-X_{min} }{(X_{max}-X_{min})}$ las numéricas para una escala de 0 a 1 igual que las dummy.


- Aunque las variables categóricas la mayoría ya tienen una estructura 0,1  hay 3 variables que son numéricas pero con más de 2 niveles lo que se requiere crear dummy para estas.




```{python}
datos_estandar=fn.estandarizacion(datos_resultado[variables],variables_num)
datos_estandar=fn.to_dummy(datos_estandar,['CONTROL', 'HIGHDEG', 'PREDDEG' ])
```


##  Aplicación de PCA


Luego de tener una estandarización y creación de variables dummy estas se usaran para la aplicación de PCA.

```{python plot50}
variables_=list(datos_estandar)
pca_=sklearn.decomposition.PCA(len(variables_))
pca_.fit(datos_estandar[variables_])
varianza_acumulate=pd.Series(pca_.explained_variance_ratio_).cumsum()
varianza_acumulate.index=varianza_acumulate.index+1
#num_componentes=varianza_acumulate[varianza_acumulate>=prop_var].index[0]
varianza_acumulate.plot(xlabel='Número de componentes', ylabel='proporción de varibilidad acumulada',grid=True,marker='o')
plt.show()
```

<figcaption> Fig 3: Variabilidad acumulada por componentes.  </figcaption>

- En la Fig 3 vemos que las 2 primeras componentes explican casi un 60% de variabilidad de las variables seleccionadas, 4 vairables explican casi un 90% de variabilidad de los datos. 


```{python}
pca_final=sklearn.decomposition.PCA(4)
pca_final.fit(datos_estandar[list(datos_estandar)])
datos_pca=pd.DataFrame(pca_final.fit_transform(datos_estandar[variables_]))
```





```{python plot6}
grafico_comparativo_num = sns.PairGrid(datos_pca )
grafico_comparativo_num=grafico_comparativo_num.map_diag(sns.kdeplot)
grafico_comparativo_num=grafico_comparativo_num.map_upper(sns.scatterplot)
grafico_comparativo_num.map_lower(sns.kdeplot)
```

<figcaption> Fig 4: Disperción de las 4 PCA  </figcaption>

En la Fig 4 se observa que en las componentes principales no se observa un número de grupos ideal, pero si se observa que hay una tendencia de agrupacion en los datos. 


## Optimizacion de clusteres

Para la creación de cluster se usaron varios metodos pero el que mostro mejores resultados fue el k-means, por lo anterior solo se mostrara los resultados con este metodo.

Entrenamos el modelo con diferentes numeros de clusters y almacenamos diferentes tipos de scoring, como son: el sse, score k means, score espectral y score birch. 

```{python tags=c()}
# Historia=[]
sse=[]
score_kmeans=[]
score_spectral=[]
score_bir=[]
#from tqdm import tqdm
num_cluster=10
for i in range(2,num_cluster):
    # k means
    Historia=sklearn.cluster.KMeans(n_clusters=i, random_state=42).fit(datos_pca)
    sse.append(Historia.inertia_) # sse kmeans
    score = sklearn.metrics.silhouette_score(datos_pca, Historia.labels_)
    score_kmeans.append(score)
    labels_temp=fn.spectral(datos_pca,list(datos_pca),i, 123 )
    score_spectral.append(sklearn.metrics.silhouette_score(datos_pca, labels_temp))
    bir_=sklearn.cluster.Birch(n_clusters=i).fit(datos_pca)
    labels_temp=bir_.labels_
    score_bir.append(sklearn.metrics.silhouette_score(datos_pca, labels_temp))    

fig, axs = plt.subplots(2, 2)
axs[0,0].plot(range(2,num_cluster ), sse,marker='o')
axs[0,0].set(xlabel="Number of Clusters",ylabel="SSE kmeans")
axs[0,1].plot(range(2, num_cluster), score_kmeans,marker='o')
axs[0,1].set(xlabel="Number of Clusters",ylabel="Score kmeans")
axs[1,0].plot(range(2, num_cluster), score_spectral,marker='o')
axs[1,0].set(xlabel="Number of Clusters",ylabel="Score spectral")
axs[1,1].plot(range(2, num_cluster), score_bir,marker='o')
axs[1,1].set(xlabel="Number of Clusters",ylabel="Score Birch")
plt.show()
```

<figcaption> Fig 5: Score clustering k-means  </figcaption>

En la Fig 5 se observa que un número minimo de cluster apropiados para las universidades podría ser 4 o mas, ya que este presenta un sse pequeño y un socre serca del 0.6 lo que es alto y a medida que se añade mas numero de cluster este factor no presenta un cambio tan significativo.

```{python tags=c()}
cluster_0=fn.kmeans(datos_pca,list(datos_pca)[0:4], 4, 133 )
datos_pca['CLUSTER']=cluster_0
datos_pca['CLUSTER']='Cluster_'+(datos_pca['CLUSTER']+1).astype(str)
datos_resultado['CLUSTER']=datos_pca['CLUSTER']
sns.scatterplot(x=0,y=1,data=datos_pca,hue='CLUSTER')
```

<figcaption> Fig 6: Disperción de los cluster con 2 componentes  </figcaption>

En la Fig 6 se observa que el metodo con 4 cluster parece funcionar bien, sin embargo se probro con mas número de cluster y no se logra una mejoría para evitar sobre agrupación un grupo ideal puede ser de 4.

## Tablas comparativas 

- En esta sección se realizara la descripción de los cluster creados para darle atributos y definir si tienen sentido o no.


```{python }
tabla_=fn.tabla_dinamica(datos_resultado,'CLUSTER',variables_num,True , 2, False)
```

<caption> Tabla 1: Descriptivos variables númericas   </caption>

`r kbl(py$tabla_, escape = F, align = "c", caption="Variables númericas",row.names = F) %>%  kable_classic_2("striped", full_width = F)`

En la Tabla 1 se puede observar como en cada variable con los 4 grupos se muestra una diferencia entre medias y cuando se uso mas cantidad de grupos no se lograba una mejor clasificación.

```{python }
tabla_=fn.tabla_dinamica(datos_resultado,'CLUSTER',variables_dum,False , 2, False)
```

<caption> Tabla 2: Descriptivos variables categóricas   </caption>


`r kbl(py$tabla_, escape = F, align = "c", caption="Variables categoricas", row.names = F) %>%  kable_classic_2("striped", full_width = F)`

Las proporciones en cada grupo parecen diferir bien entre cluster por lo que se puede dar una descripción clara de cada cluster.

# Resultados.

Según las tablas y con lo que se realizo internamente no se encontro otra segmentación que mejorara estas descripciones ya que a medida que se partian los grupos sus descriptivos no mejoraran en gran medida la diferencia entre si por lo tanto, esta es la descripción de cada cluster

## Descripciones de clusters

- **Cluster 1 :** En este grupo se encuentran 2635 instituciones educativas con un costo por matrícula cercano a 7.6mil dólares, y con una inversión por estudiante próxima a los 3.9mil dólares. En promedio estas instituciones educativas albergan 2 programas y la mayoría son privadas y con ánimo de lucro. Dentro de estas predominan las licenciaturas, que a su vez suelen ser el mayor nivel académico en estas institucions. 



- **Cluster 2:** En este grupo se encuentran 1.937 instituciones educativas con un costo de matrícula cercano a 14 mil dólares, y con una inversión próxima a 7.3 miles de dólares. En promedio estas instituciones educativas albergan 15 programas, y la mayoría son privadas sin ánimo de lucro. Dentro de estas predominan las carreras profesionales, y su máximo nivel educativo son los posgrados. 

- **Cluster 3:**  En este grupo se encuentran 2097 instituciones educativas con un costo por matrícula cercano a 5 mil dólares, y con una inversión próxima a 4.6 mil dólares. En promedio estas instituciones educativas albergan 20 programas académicos, y son públicas o privadas con ánimo de lucro. Dentro de estas predominan los grados asociados, que a su vez son el nivel educativo más alto.

- **Cluster 4:** En este grupo se encuentran 610 instituciones educativas con un costo por matrícula cercano a 6.5 mil dólares, y con una inversión próxima a 8 mil dólares. En promedio estas instituciones educativas albergan 27 programas académicos, y son públicas. Dentro de estas predominan las carreras profesional, y su nivel educativo más alto son los posgrados


# Conclusión 

Con las variables escogidas y los cluster formados se logro a una descrpción clara de cada uno de los cluster y además para el desarrollo de la app que un usuario tenga que entender demasiados cluster puede que no sea lo mas conveniente. En el caso de colombia es posible general la misma metodología con variables como número de programas, costo de matricula promedio, inversión de dinero por estudiante, tipo de universidad, y que nivel educativo ofrecen o ya sea cual predomina o el maximo nivel academico.


